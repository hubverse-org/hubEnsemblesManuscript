---
title: "[hubEnsembles]{.pkg}: Ensembling Methods in [R]{.proglang}"
format:
  jss-pdf:
    keep-tex: true
    journal:
      type: article
      cite-shortnames: true
      suppress: [title]
      include-jss-default: false
author:
  - name: Li Shandross
    affiliations:
      - name: University of Massachusetts Amherst
    orcid: 0009-0008-1348-1954
    attributes:
      equal-contributor: true
    email: lshandross@umass.edu
  - name: Emily Howerton
    affiliations:
      - The Pennsylvania State University
    orcid: 0000-0002-0639-3728
    attributes:
      equal-contributor: true
  - name: Lucie Contamin
    affiliations:
      - University of Pittsburgh
    orcid: 0000-0001-5797-1279
  - name: Harry Hochheiser
    affiliations:
      - University of Pittsburgh
    orcid: 0000-0001-8793-9982
  - name: Anna Krystalli
    affiliations:
      - R-RSE SMPC
    orcid: 0000-0002-2378-4915
  - name: |
      | Consortium of 
      | Infectious Disease Modeling Hubs
    affiliations:
      - A list of authors and their affiliations appears at the end of the paper
  - name: Nicholas G. Reich
    affiliations:
      - University of Massachusetts Amherst
    orcid: 0000-0003-3503-9899
  - name: Evan L. Ray
    affiliations:
      - University of Massachusetts Amherst
    orcid: 0000-0003-4035-0243
execute:
  echo: true
  warning: false
  message: false
bibliography: references.bib
abstract: |
  Combining predictions from multiple models into an ensemble is a widely used practice across many fields with demonstrated performance benefits. The [R]{.proglang} package [hubEnsembles]{.pkg} provides a flexible framework for ensembling various types of predictions, including point estimates and probabilistic predictions. A range of common methods for generating ensembles are supported, including weighted averages, quantile averages, and linear pools. The [hubEnsembles]{.pkg} package fits within a broader framework of open-source software and data tools called the "hubverse", which facilitates the development and management of collaborative modelling exercises.

keywords: [multiple models, aggregation, forecast, prediction]
keywords-formatted: [multiple models, aggregation, forecast, prediction]
---

```{r setup}
#| echo: FALSE
library(hubEnsemblesManuscript)
library(hubEnsembles)
library(hubUtils)
library(hubExamples)
library(hubVis)
library(readr)
library(dplyr)
library(tidyr)
library(reshape2)
library(lubridate)
library(patchwork)
library(here)
library(ggplot2)
library(reshape2)
library(cowplot)
```

## Introduction {#sec-intro}

Predictions of future outcomes are essential to planning and decision making, yet generating reliable predictions of the future is challenging. One method for overcoming this challenge is combining predictions across multiple, independent models. These combination methods (also called aggregation or ensembling) have been repeatedly shown to produce predictions that are more accurate [@clemen1989; @timmermann2006] and more consistent [@hibon2005] than individual models. Because of the clear performance benefits, multi-model ensembles are commonplace across fields, including weather [@alley2019], climate [@tebaldi2007], and economics [@aastveit2018]. More recently, multi-model ensembles have been used to improve predictions of infectious disease outbreaks [@viboud2018; @johansson2019; @mcgowan2019; @reich_accuracy_2019; @cramer2022].

In the rapidly growing field of outbreak forecasting, there are many proposed methods for generating ensembles. Generally, these methods differ in at least one of two ways: (1) the function used to combine or "average" predictions, and (2) how predictions are weighted when performing the combination. No one method is universally "the best"; a simple average of predictions works surprisingly well across a range of settings [@mcgowan2019; @paireau_ensemble_2022; @ray_comparing_2023] for established theoretical reasons [@winkler2015]. However, more complex approaches have also been shown to have benefits in some settings [@yamana_superensemble_2016; @ray_prediction_2018; @reich_accuracy_2019; @colon-gonzalez_probabilistic_2021]. Here, we present the [hubEnsembles]{.pkg} package, which provides a flexible framework for generating ensemble predictions from multiple models. Complementing other software for combining predictions from multiple models (e.g., @pedregosa_scikit-learn_2011; @weiss2019; @bosse_stackr_2023; @couch_stacks_2023), [hubEnsembles]{.pkg} supports multiple types of predictions, including point estimates and different kinds of probabilistic predictions. Throughout, we will use the term "prediction" to refer to any kind of model output that may be combined including a forecast, a scenario projection, or a parameter estimate.

The [hubEnsembles]{.pkg} package is part of the "hubverse" collection of open-source software and data tools. The hubverse project facilitates the development and management of collaborative modelling exercises [@hubverse_docs]. The broader hubverse initiative is motivated by the demonstrated benefits of collaborative hubs [@reich2022; @borchering_public_2023], including performance benefits of multi-model ensembles and the desire for standardization across such hubs. In this paper, we focus specifically on the functionality encompassed in [hubEnsembles]{.pkg}. We provide an overview of the methods implemented, including mathematical definitions and properties (@sec-defs) as well as implementation details (@sec-implementation), a basic demonstration of functionality with simple examples (@sec-simple-ex), and a more in-depth analysis using real influenza forecasts (@sec-case-study) that motivates a discussion and comparison of the various methods (@sec-conclusions).

## Mathematical definitions and properties of ensemble methods {#sec-defs}

The [hubEnsembles]{.pkg} package supports both point predictions and probabilistic predictions of different formats. A point prediction gives a single estimate of a future outcome while a probabilistic prediction provides an estimated probability distribution over a set of future outcomes. We use $N$ to denote the total number of individual predictions that the ensemble will combine. For example, these predictions will often be produced by different statistical or mathematical models, and $N$ is the total number of models that have provided predictions. Individual predictions will be indexed by the subscript $i$. Optionally, the package allows for calculating ensembles that use a weight $w_i$ for each prediction; we define the set of model-specific weights as $\pmb{w} = \{w_i | i \in 1, ..., N\}$. Informally, predictions with a larger weight have a greater influence on the ensemble prediction, though the details of this depend on the ensemble method (described further below).

For a set of $N$ point predictions, $\pmb{p} = \{p_i|i \in 1, ..., N\}$, each from a distinct model $i$, the [hubEnsembles]{.pkg} package can compute an ensemble of these predictions

$$
p_E = C(\pmb{p}, \pmb{w}) 
$$

using any function $C$ and any set of model-specific weights $\pmb{w}$. For example, an arithmetic average of predictions yields $p_E = \sum_{i=1}^Np_iw_i$, where the weights are non-negative and sum to 1. If $w_i = 1/N$ for all $i$, all predictions will be equally weighted. This framework can also support more complex functions for aggregation, such as a (weighted) median or geometric mean.

For probabilistic predictions, there are two commonly used classes of methods to average or ensemble multiple predictions: quantile averaging (also called a Vincent average [@vincent1912]) and probability averaging (also called a distributional mixture or linear opinion pool [@stone1961]) [@lichtendahl2013]. To define these two classes of methods, let $F(x)$ be a cumulative density function (CDF) defined over values $x$ of the target variable for the prediction, and $F^{-1}(\theta)$ be the corresponding quantile function defined over quantile levels $\theta \in [0, 1]$. Throughout this article, we may refer to $x$ as either a 'value of the target variable' or a 'quantile' depending on the context, and similarly we may refer to $\theta$ as either a 'quantile level' or a '(cumulative) probability'. Additionally, we will use $f(x)$ to denote a probability mass function (PMF) for a prediction of a discrete variable or a discretization (such as binned values) of a continuous variable.

The quantile average combines a set of quantile functions, $\mathcal{Q} = \{F_i^{-1}(\theta)| i \in 1,...,N \}$, with a given set of weights, $\pmb{w}$, as $$
F^{-1}_Q(\theta) = C_Q(\mathcal{Q}, \pmb{w}) = \sum_{i = 1}^Nw_iF^{-1}_i(\theta).
$$

This computes the average value of predictions across different models for each fixed quantile level $\theta$. For a normal distribution or any distribution with a shape and scale parameter, the resulting quantile average will be the same type of distribution, with shape and scale parameters that are the average of the shape and scale parameters from the individual distributions (@fig-example-quantile-average-and-linear-pool, panel B). In other words, this method inteprets the predictive probability distributions that are being combined as uncertain estimates of a single true distribution. It is also possible to use other combination functions, such as a weighted median, to combine quantile predictions.

The probability average or linear pool is calculated by averaging probabilities across predictions for a fixed value of the target variable, $x$. In other words, for a set $\mathcal{F} = \{F_i(x)| i \in 1,...,N \}$ containing the values of CDFs at the point $x$ and weights $\pmb{w}$, the linear pool is calculated as

$$
F_{LOP}(x) = C_{LOP}(\mathcal{F}, \pmb{w}) = \sum_{i = 1}^Nw_iF_i(x). 
$$

For a set of PMF values, $\{f_i(x)|i \in 1, ..., N\}$, the linear pool can be equivalently calculated: $f_{LOP}(x) = \sum_{i = 1}^N w_i f_i(x)$. Statistically this amounts to a mixture of the probability distributions, and the resulting probability distribution can be interpreted as one where the constituent probability distributions represent alternative predictions of the future, each of which has a probability $w_i$ of being the true one. For a visual depiction of these equations, see @fig-example-quantile-average-and-linear-pool below.

The different averaging methods for probabilistic predictions yield different properties of the resulting ensemble distribution. For example, the variance of the linear pool is $\sigma^2_{LOP} = \sum_{i=1}^Nw_i\sigma_i^2 + \sum_{i=1}^Nw_i(\mu_i-\mu_{LOP})^2$, where $\mu_i$ is the mean and $\sigma^2_i$ is the variance of individual prediction $i$, and although there is no closed-form variance for the quantile average, the variance of the quantile average will always be less than or equal to that of the linear pool [@lichtendahl2013]. Both methods generate distributions with the same mean, $\mu_Q = \mu_{LOP} = \sum_{i=1}^Nw_i\mu_i$, which is the mean of individual model means [@lichtendahl2013]. The linear pool method preserves variation between individual models, whereas the quantile average cancels away this variation under the assumption it constitutes sampling error [@howerton2023].

## Model implementation details {#sec-implementation}

To understand how these methods are implemented in [hubEnsembles]{.pkg}, we first must define the conventions employed by the hubverse and its packages for representing and working with model predictions. We begin with a short overview of concepts and conventions needed to utilize the [hubEnsembles]{.pkg} package, supplemented by example predictions provided by the hubverse, then explain the implementation of the two ensembling functions included in the package, `simple_ensemble()` and `linear_pool()`.

### Hubverse terminology and conventions

A central concept in the hubverse effort is "model output". Model output is a specially formatted tabular representation of predictions. Each row represents a single, unique prediction with each column providing information about what is being predicted, its scope, and its value. Per hubverse convention, each column serves one of three purposes: (i) denote which model has produced the prediction (called the "model ID"), (ii) provide details about what is being predicted (called the "task IDs"), or (iii) specify the prediction itself and how it is represented (called the "model output representation") [@hubverse_docs].

Predictions are assumed to be generated by distinct models, typically developed and run by a modeling team of one or more individuals. Each model should have a unique identifier that is stored in the `model_id` column. Then, the details of the outcome being predicted can be stored in a series of task ID columns, the second type of column. These task ID columns may also include additional information, such as any conditions or assumptions that were used to generate the predictions [@hubverse_docs]. For example, short-term forecasts of incident influenza hospitalizations in the US at different locations and amounts of time in the future might represent this information using a `target` column with the value "wk inc flu hosp", a `location` column identifying the location being predicted (not shown), a `reference_date` column with the "starting point" of the forecasts (not shown), and a `horizon` column with the number of steps ahead that the forecast is predicting relative to the `reference_date` (@tbl-example-forecasts). All these variables make up the task ID columns.

```{r example-forecasts, echo=FALSE}
#| label: tbl-example-forecasts
#| tbl-cap: "Example of forecasts for incident influenza hospitalizations,
#| formatted according to hubverse standards. Quantile predictions for the
#| median and 50% prediction intervals from a single model are shown for four
#| distinct horizons. The `location` and `reference_date` columns have been
#| omitted for brevity; all forecasts in this example were made on 2022-12-17
#| for Massachusetts. These predictions are a modified subset of the
#| `forecast_outputs` data provided by the [hubExamples]{.pkg} package."

abbr_fc_col <- formatted_fc_col <- c("model_id", "target", "horizon", "output_type", "output_type_id", "value")
for (i in 1: length(abbr_fc_col)) formatted_fc_col[i] <- paste0("`", abbr_fc_col[i], "`")

hubExamples::forecast_outputs |>
  dplyr::filter(
    model_id == "MOBS-GLEAM_FLUH", 
    location == "25", 
    reference_date == "2022-12-17",
    output_type == "quantile",
    output_type_id %in% c(0.25, 0.5, 0.75)
  ) |>
  dplyr::mutate(value=round(value, 2), 
                model_id = "model-X") |>
  dplyr::select(all_of(abbr_fc_col)) |>
  dplyr::arrange(horizon) |>
  knitr::kable(col.names = formatted_fc_col)
```

Alternatively, longer-term scenario projections may require different task ID columns. For example, projections of incident COVID-19 cases in the US at different locations, amounts of time in the future, and under different assumed conditions may use a `target` column of "inc case", a `location` column specifying the location being predicted (not shown), an `origin_date` column specifying the date on which the projections were made (not shown), a `horizon` column describing the number of steps ahead that the projection is predicting relative to the `origin_date`, and a `scenario_id` column denoting the future conditions that were modeled and are projected to result in the specified number of incident cases (@tbl-example-scenarios). Different modeling efforts may use different sets of task ID columns and values to specify their prediction goals, or may simply choose distinct names to represent the same concept (e.g., `reference_date` versus `origin_date` for a date task ID). Additional examples of task ID variables are available on the hubverse documentation website [@hubverse_docs].

\newpage

```{r example-scenarios, echo=FALSE}
#| label: tbl-example-scenarios
#| tbl-cap: "Example of scenario projections for incident COVID-19 cases,
#| formatted according to hubverse standards. Quantile predictions for the
#| median and 50% prediction intervals from a single model are shown for four
#| distinct scenarios. The `location` and `origin_date` columns have been
#| omitted for brevity; all forecasts in this example were made on 2021-03-07
#| for the US. These predictions are a modified subset of the `scenario_outputs`
#| data provided by the [hubExamples]{.pkg} package."

abbr_scn_col <- formatted_scn_col <- c("model_id", "target", "horizon", "scenario_id", "output_type", "output_type_id", "value")
for (i in 1: length(abbr_scn_col)) formatted_scn_col[i] <- paste0("`", abbr_scn_col[i], "`")

hubExamples::scenario_outputs |>
  dplyr::filter(
    model_id == "HUBuni-simexamp", 
    location == "US", 
    horizon == "26", 
    output_type_id %in% c(0.25, 0.5, 0.75)
  ) |>
  dplyr::select(all_of(abbr_scn_col)) |>
  dplyr::mutate(model_id = "model-Y", 
                scenario_id = substr(scenario_id,1,1)) |>
  knitr::kable(col.names = formatted_scn_col)
```

The third group of columns in model output specify the model predictions and details about how the predictions are represented. This "model output representation" includes the predicted values along with metadata that specifies how the predictions are conveyed and always consists of the same three columns: (1) `output_type`, (2) `output_type_id`, and (3) `value`. The `output_type` column defines how the prediction is represented and may be one of `"mean"` or `"median"` for point predictions, or `"quantile"`, `"cdf"`, `"pmf"`, or `"sample"` for probabilistic predictions. (The sample output type is not yet directly supported by the [hubEnsembles]{.pkg} package and should be converted to another output type before computing an ensemble.) The `output_type_id` provides additional identifying information for a prediction and is specific to the particular `output_type` (see @tbl-model-output-rep). For quantile predictions, the `output_type_id` is a numeric value between 0 and 1 specifying the cumulative probability associated with the quantile prediction. In the notation we defined above, the `output_type_id` corresponds to $\theta$ and the `value` is the quantile prediction $F^{-1}(\theta)$. For CDF or PMF predictions, the `output_type_id` is the target variable value $x$ at which the cumulative distribution function or probability mass function for the predictive distribution should be evaluated, and the `value` column contains the predicted $F(x)$ or $f(x)$, respectively. Requirements for the values of the `output_type_id` and `value` columns associated with each valid output type are summarized in @tbl-model-output-rep.

This representation of predictive model output is codified by the `model_out_tbl` S3 class in the [hubUtils]{.pkg} package, one of the foundational hubverse packages. Although this S3 class is required for all [hubEnsembles]{.pkg} functions, model predictions in other formats can easily be transformed using the `as_model_out_tbl()` function from [hubUtils]{.pkg}. An example of this transformation is provided in @sec-case-study.

| `output_type` | `output_type_id`                                                                            | `value`                                                                                                                                                       |
|:-----------------|:----------------------|:------------------------------|
| `mean`        | NA (not used for mean predictions)                                                          | Numeric: The mean of the predictive distribution                                                                                                              |
| `median`      | NA (not used for median predictions)                                                        | Numeric: The median of the predictive distribution                                                                                                            |
| `quantile`    | Numeric between 0.0 and 1.0: A quantile level                                               | Numeric: The quantile of the predictive distribution at the quantile level specified by the `output_type_id`                                                  |
| `cdf`         | String or numeric naming a possible value of the target variable | Numeric between 0.0 and 1.0: The cumulative probability of the predictive distribution at the value of the outcome variable specified by the `output_type_id` |
| `pmf`         | String naming a possible category of a discrete outcome variable                            | Numeric between 0.0 and 1.0: The probability mass of the predictive distribution when evaluated at a specified level of a discrete outcome variable           |
| `sample`*      | Integer or string specifying the sample index                                               | Numeric: A sample from the predictive distribution                                                                                                            |

: A table summarizing how the model output representation columns are used for predictions of different output types. *Sample output types are not yet supported by the [hubEnsembles]{.pkg} package. Adapted from <https://docs.hubverse.io/en/latest/user-guide/model-output.html#formats-of-model-output> {#tbl-model-output-rep}

### Ensemble functions in hubEnsembles {#sec-ens-fns}

The [hubEnsembles]{.pkg} package includes two functions that perform ensemble calculations: `simple_ensemble()`, which applies some function to each model prediction, and `linear_pool()`, which computes an ensemble using the linear opinion pool method. In the following sections, we outline the implementation details for each function and how these implementations correspond to the statistical ensembling methods described in @sec-defs. A short description of the calculation performed by each function is summarized by output type in @tbl-fns-by-output-type.

#### Simple ensemble {#sec-simple-ensemble}

The `simple_ensemble()` function directly computes an ensemble from component model outputs by combining them via some function ($C$) within each unique combination of task ID variables, output types, and output type IDs. This function can be used to summarize predictions of output types mean, median, quantile, CDF, and PMF. The mechanics of the ensemble calculations are the same for each of the output types, though the resulting statistical ensembling method differs for different output types (@tbl-fns-by-output-type).

| `output_type` | `simple_ensemble(..., agg_fun=mean)`                                                          | `linear_pool()`                                                                                                                                                          |
|------------------|---------------------------|---------------------------|
| `mean`        | mean of individual model means                                                                | mean of individual model means                                                                                                                                           |
| `median`      | mean of individual model medians                                                              | NA                                                                                                                                                                       |
| `quantile`    | mean of individual model target variable values at each quantile level, $F^{-1}_Q(\theta)$    | quantile of the distribution obtained by computing the mean of estimated individual model cumulative probabilities at each target variable value, $F^{-1}_{LOP}(\theta)$ |
| `cdf`         | mean of individual model cumulative probabilities at each target variable value, $F_{LOP}(x)$ | mean of individual model cumulative probabilities at each target variable value, $F_{LOP}(x)$                                                                            |
| `pmf`         | mean of individual model bin or category probabilities at each target variable value, $f_{LOP}(x)$        | mean of individual model bin or category probabilities at each target variable value, $f_{LOP}(x)$                                                                                   |

: Summary of ensemble function calculations for each output type. The ensemble function determines the operation that is performed, and in the case of probabilistic output types (quantile, CDF, PMF), this also determines what ensemble distribution is generated (quantile average, $F_{Q}^{-1}(\theta)$, or linear pool, $F_{LOP}(x)$). The ensembled predictions are returned in the same output type as the inputs. Thus, the output type determines how the resulting ensemble distribution is summarized (as a quantile, $F^{-1}(\theta)$, cumulative probability, $F(x)$, or probability $f(x)$). Estimating individual model cumulative probabilities is required to compute a `linear_pool()` for predictions of `quantile` output type; see @sec-linear-pool on the linear pool operation for details. In the case of `simple_ensemble()`, we show the calculations for the default case where `agg_fun = mean`; however, if another aggregation function is chosen (e.g., `agg_fun = median`), that calculation would be performed instead. For example, `simple_ensemble(..., agg_fun = median)` applied to predictions of mean output type would return the median of individual model means. {#tbl-fns-by-output-type}{tbl-colwidths="\[14,50,36\]"}

By default, `simple_ensemble()` uses the mean for the aggregation function $C$ and equal weights for all models. For point predictions with a mean or median output type, the resulting ensemble prediction is an equally weighted average of the individual models' predictions. For probabilistic predictions in a quantile format, by default `simple_ensemble()` calculates an equally weighted average of individual model target variable values at each quantile level, which is equivalent to a quantile average. For model outputs in a CDF or PMF format, by default `simple_ensemble()` computes an equally weighted average of individual model (cumulative or bin) probabilities at each target variable value, which is equivalent to the linear pool method.

Any aggregation function $C$ may be specified by the user. For example, a median ensemble may also be created by specifying `median` as the aggregation function, or a custom function may be passed to the `agg_fun` argument to create other ensemble types. Similarly, model weights can be specified to create a weighted ensemble.

#### Linear pool {#sec-linear-pool}

The `linear_pool()` function implements the linear opinion pool (LOP) method for ensembling predictions. Currently, this function can be used to combine predictions with output types mean, quantile, CDF, and PMF. Unlike `simple_ensemble()`, this function handles its computation differently based on the output type. For the CDF, PMF, and mean output types, the linear pool method is equivalent to calling `simple_ensemble()` with a mean aggregation function (see @tbl-fns-by-output-type), since `simple_ensemble()` produces a linear pool prediction (an average of individual model cumulative or bin probabilities).

However, implementation of LOP is less straightforward for the quantile output type. This is because LOP averages cumulative probabilities at each value of the target variable, but the predictions are given as quantiles (on the scale of the target variable) for fixed quantile levels. The value for these quantile predictions will generally differ between models; hence, we are typically not provided cumulative probabilities at the same values of the target variable for all component predictions. This lack of alignment between cumulative probabilities for the same target variable values impedes computation of LOP from quantile predictions and is illustrated in panel A of @fig-example-quantile-average-and-linear-pool.

```{r example-quantile-average-and-linear-pool, echo=FALSE}
#| label: fig-example-quantile-average-and-linear-pool
#| fig-cap: "(Panel A) Example of quantile output type predictions. Solid
#| points show model output collected for seven fixed quantile levels
#| ($\\theta$ = 0.01, 0.1, 0.3, 0.5, 0.7, 0.9, and 0.99) from two distributions
#| ($N(100, 10)$ in purple and $N(120, 5)$ in green), with the underlying
#| cumulative distribution functions (CDFs) shown with curves. The y-axis ticks
#| show each of the fixed quantile levels. The associated values for each fixed
#| quantile level do not align across distributions (vertical lines).
#| (Panel B) Quantile average ensemble, which is calculated by
#| averaging values for each fixed quantile level (represented by horizontal
#| dashed gray lines). The distributions and corresponding model outputs from
#| panel A are re-plotted and the black line shows the resulting quantile
#| average ensemble. Inset shows corresponding probability density functions
#| (PDFs). (Panel C) Linear pool ensemble, which is calculated by averaging
#| cumulative probabilities for each fixed value (represented by vertical
#| dashed gray lines). The distributions and corresponding model outputs from
#| panel A are re-plotted. To calculate the linear pool in this case, where
#| model outputs are not defined for the same values, the model outputs are used
#| to interpolate the full CDF for each distribution from which quantiles can be
#| extracted for fixed values (shown with open circles). The black line shows
#| the resulting linear pool average ensemble. Inset shows corresponding PDFs."
#| fig-width: 10
#| fig-height: 4

# set quantiles and values over which to define distributions
quantile_probs <- c(0.01, seq(from = 0.1, to = 0.9, by = 0.2), 0.99)
lop_example_x <- seq(85, 135, 12)
x <- unique(c(seq(from = 0.0, to = 400.0, length = 1001), lop_example_x))

# distribution 1 ~ N(100, 10)
mean1 <- 100
sd1 <- 10
# distribution 2 ~ N(120, 5)
mean2 <- 120
sd2 <- 5
# distribution colors
dist_colors <- viridisLite::viridis(2, end = 0.9)

# cdfs defined for fixed quantiles
cdf_defined_on_quantiles <-
  data.frame(
    quantile = rep(quantile_probs, 3),
    value = c(
      qnorm(quantile_probs, mean1, sd1),
      qnorm(quantile_probs, mean2, sd2),
      qnorm(quantile_probs, mean(c(mean1, mean2)), mean(c(sd1, sd2)))
    ),
    distribution = c(
      rep("A", length(quantile_probs)),
      rep("B", length(quantile_probs)),
      rep("Q", length(quantile_probs))
    )
  )

# cdfs defined for fixed values
cdf_defined_on_values <-
  data.frame(
    value = rep(x, 4),
    quantile = c(
      pnorm(x, mean1, sd1), pnorm(x, mean2, sd2),
      rowMeans(cbind(pnorm(x, mean1, sd1), pnorm(x, mean2, sd2))),
      pnorm(x, mean(c(mean1, mean2)), mean(c(sd1, sd2)))
    ),
    distribution = c(
      rep("A", length(x)), rep("B", length(x)),
      rep("LOP", length(x)), rep("Q", length(x))
    )
  )

# pdfs for all distributions
pdfs_defined_on_values <-
  data.frame(
    value = rep(x, 4),
    prob = c(
      dnorm(x, mean = mean1, sd = sd1),
      dnorm(x, mean = mean2, sd = sd2),
      rowMeans(cbind(dnorm(x, mean1, sd1), dnorm(x, mean2, sd2))),
      dnorm(x, mean(c(mean1, mean2)), mean(c(sd1, sd2)))
    ),
    distribution = c(
      rep("A", length(x)), rep("B", length(x)),
      rep("LOP", length(x)), rep("Q", length(x))
    )
  )

### panel A
pa <- ggplot(
  data = dplyr::filter(
    cdf_defined_on_values,
    distribution %in% c("A", "B")
  ),
  aes(x = value, y = quantile, color = distribution)
) +
  geom_line(linewidth = 0.8, alpha = 0.3) +
  geom_segment(
    data = dplyr::filter(
      cdf_defined_on_quantiles,
      distribution %in% c("A", "B")
    ),
    aes(x = value, xend = value, y = 0, yend = quantile),
    alpha = 0.5
  ) +
  geom_point(data = dplyr::filter(
    cdf_defined_on_quantiles,
    distribution %in% c("A", "B")
  )) +
  coord_cartesian(xlim = c(75, 140)) +
  labs(
    x = "value", y = "cumulative probability (quantile)",
    subtitle = "quantile predictions from two distributions"
  ) +
  scale_color_manual(
    name = "distributions",
    labels = c("N(100, 10)", "N(120, 5)"),
    values = dist_colors
  ) +
  scale_y_continuous(
    breaks = quantile_probs, limits = c(0, 1),
    expand = c(0, 0)
  ) +
  theme_bw() +
  theme(
    legend.position = "bottom",
    panel.grid.minor = element_blank(),
    panel.grid.major.x = element_blank()
  )

### panel B
pb <- ggplot(
  data = dplyr::filter(cdf_defined_on_values, distribution != "LOP"),
  mapping = aes(x = value, y = quantile)
) +
  geom_segment(
    data = dplyr::filter(
      cdf_defined_on_quantiles,
      distribution != "LOP"
    ) |>
      reshape2::dcast(quantile ~ distribution, value.var = "value"),
    mapping = aes(x = A, xend = B, y = quantile, yend = quantile),
    color = "darkgray", linetype = "dashed"
  ) +
  geom_line(mapping = aes(
    alpha = distribution, color = distribution,
    linewidth = distribution
  )) +
  geom_point(
    data = dplyr::filter(
      cdf_defined_on_quantiles,
      distribution %in% c("A", "B")
    ),
    mapping = aes(color = distribution, size = distribution)
  ) +
  coord_cartesian(xlim = c(75, 135)) +
  labs(x = "value", y = "", subtitle = "quantile average ensemble") +
  scale_alpha_manual(values = c(0.3, 0.3, 1)) +
  scale_color_manual(values = c(dist_colors, "black")) +
  scale_linewidth_manual(values = c(0.8, 0.8, 1.2)) +
  scale_size_manual(values = c(1.2, 1.2, 1.7)) +
  scale_y_continuous(
    breaks = quantile_probs, limits = c(0, 1),
    expand = c(0, 0)
  ) +
  theme_bw() +
  theme(
    legend.position = "none", panel.grid.minor = element_blank(),
    panel.grid.major.x = element_blank()
  )

pb_inset <- ggplot(
  data = dplyr::filter(
    pdfs_defined_on_values,
    distribution != "LOP"
  ),
  mapping = aes(x = value, y = prob, color = distribution)
) +
  geom_line(aes(alpha = distribution, linewidth = distribution)) +
  coord_cartesian(xlim = c(75, 135)) +
  scale_alpha_manual(values = c(0.8, 0.8, 1)) +
  scale_color_manual(values = c(dist_colors, "black")) +
  scale_linewidth_manual(values = c(0.8, 0.8, 1.2)) +
  theme_bw() +
  theme(
    axis.text.y = element_blank(), axis.ticks.y = element_blank(),
    axis.title = element_blank(), legend.position = "none",
    panel.grid = element_blank()
  )

pb_fin <- cowplot::ggdraw(pb) +
  cowplot::draw_plot(
    pb_inset + theme(plot.margin = unit(
      0.01 * c(1, 1, 1, 1),
      "cm"
    )),
    .19, .6, .3, .3,
    hjust = 0, vjust = 0
  )


### panel C
pc <- ggplot(
  data = cdf_defined_on_values |> dplyr::filter(distribution != "Q"),
  mapping = aes(x = value, y = quantile)
) +
  geom_segment(
    data = cdf_defined_on_values |>
      dplyr::filter(distribution != "Q", value %in% lop_example_x) |>
      reshape2::dcast(value ~ distribution, value.var = "quantile"),
    mapping = aes(x = value, xend = value, y = A, yend = B),
    color = "darkgray", linetype = "dashed"
  ) +
  geom_line(mapping = aes(
    alpha = distribution, color = distribution,
    linewidth = distribution
  )) +
  geom_point(
    data = dplyr::bind_rows(
      cdf_defined_on_values |>
        dplyr::filter(
          distribution %in% c("A", "B"),
          value %in% lop_example_x
        ) |>
        dplyr::mutate(point_type = "interpolation"),
      dplyr::filter(
        cdf_defined_on_quantiles,
        distribution %in% c("A", "B")
      ) |>
        dplyr::mutate(point_type = "model output")
    ),
    mapping = aes(
      color = distribution, size = distribution,
      shape = point_type
    ),
  ) +
  coord_cartesian(xlim = c(75, 135)) +
  guides(alpha = "none", color = "none", linewidth = "none", size = "none") +
  labs(x = "value", y = "", subtitle = "linear pool ensemble") +
  scale_alpha_manual(values = c(0.3, 0.3, 1)) +
  scale_color_manual(values = c(dist_colors, "black")) +
  scale_linewidth_manual(values = c(0.8, 0.8, 1.2)) +
  scale_shape_manual(values = c(1, 19), name = "") +
  scale_size_manual(values = c(1.2, 1.2, 1.9)) +
  scale_y_continuous(
    breaks = quantile_probs, limits = c(0, 1),
    expand = c(0, 0)
  ) +
  theme_bw() +
  theme(
    legend.position = "bottom", panel.grid.minor = element_blank(),
    panel.grid.major.x = element_blank()
  )

pc_inset <- ggplot(
  data = pdfs_defined_on_values |>
    dplyr::filter(distribution != "Q"),
  mapping = aes(x = value, y = prob, color = distribution)
) +
  geom_line(aes(alpha = distribution, linewidth = distribution)) +
  coord_cartesian(xlim = c(75, 135)) +
  scale_alpha_manual(values = c(0.8, 0.8, 1)) +
  scale_color_manual(values = c(dist_colors, "black")) +
  scale_linewidth_manual(values = c(0.8, 0.8, 1.2)) +
  theme_bw() +
  theme(
    axis.text.y = element_blank(), axis.ticks.y = element_blank(),
    axis.title = element_blank(), legend.position = "none",
    panel.grid = element_blank()
  )

pc_fin <- cowplot::ggdraw(pc + theme(legend.position = "none")) +
  cowplot::draw_plot(
    pc_inset + theme(plot.margin = unit(
      0.01 * c(1, 1, 1, 1),
      "cm"
    )),
    .19, .6, .3, .3,
    hjust = 0, vjust = 0
  )

# combine into final figure
l <- cowplot::get_plot_component(pa, "guide-box", return_all = TRUE)[[3]]
l2 <- cowplot::get_plot_component(pc, "guide-box", return_all = TRUE)[[3]]

cowplot::plot_grid(
  cowplot::plot_grid(
    pa + theme(legend.position = "none"), pb_fin, pc_fin,
    labels = LETTERS[1:3], nrow = 1, rel_widths = c(0.34, 0.33, 0.33)
  ),
  cowplot::plot_grid(
    NULL, l, l2, NULL,
    nrow = 1, rel_widths = c(0.175, 0.325, 0.325, 0.175)
  ),
  ncol = 1, rel_heights = c(0.95, 0.05)
)
```

Given that LOP cannot be directly calculated from quantile predictions, we must first obtain an estimate of the CDF for each component distribution from the provided quantiles, combine the CDFs, then calculate the quantiles using the ensemble's CDF. We perform this calculation in three main steps, assisted by the [distfromq]{.pkg} package [@distfromq] for the first two:

1.  Interpolate and extrapolate from the provided quantiles for each component model to obtain an estimate of the CDF of that particular distribution.
2.  Draw samples from each component model distribution. To reduce Monte Carlo variability, we use quasi-random samples corresponding to quantiles of the estimated distribution [@niederreiter1992quasirandom].
3.  Pool the samples from all component models and extract the desired quantiles.

For step 1, functionality in the [distfromq]{.pkg} package uses a monotonic cubic spline for interpolation on the interior of the provided quantiles. The user may choose one of several distributions to perform extrapolation of the CDF tails. These include normal, lognormal, and cauchy distributions, with "normal" set as the default. A location-scale parameterization is used, with separate location and scale parameters chosen in the lower and upper tails so as to match the two most extreme quantiles. The sampling process described in steps 2 and 3 approximates the linear pool calculation described in @sec-defs.

## Basic demonstration of functionality {#sec-simple-ex}

In this section, we provide a simple example to illustrate the two main functions in [hubEnsembles]{.pkg}, `simple_ensemble()` and `linear_pool()`.

### Example data: a forecast hub

```{r, cols.print=12}
#| label: tbl-example-model-outputs
#| echo: false
#| tbl-cap: "Example model output for forecasts of incident influenza
#| hospitalizations. A subset of example model output is shown: 1-week ahead
#| quantile forecasts made on 2022-12-17 for Massachusetts from three distinct
#| models; only the median and 5th, 25th, 75th and 95th quantiles are displayed. 
#| The `location`, `reference_date` and `target_end_date`
#| columns have been omitted for brevity. This example data is
#| provided in the [hubExamples]{.pkg} package."
hubExamples::forecast_outputs |>
  dplyr::filter(
    output_type %in% c("quantile", "median"),
    output_type_id %in% c(0.05, 0.25, 0.75, 0.75, 0.95, NA),
    reference_date == "2022-12-17",
    location == "25",
    horizon == 1
  ) |>
  dplyr::select(all_of(abbr_fc_col)) |>
  knitr::kable(col.names = formatted_fc_col)
```

We will use an example hub provided by the hubverse to demonstrate the functionality of the [hubEnsembles]{.pkg} package [@hubverse_docs]. This hub was generated using modified forecasts from the FluSight forecasting challenge (discussed in further detail in @sec-case-study). The example hub includes both example model output data and target data (sometimes known as "truth" data), which are stored in the [hubExamples]{.pkg} package as data objects named `forecast_outputs` and `forecast_target_ts`. Note that this model outputs data contain only a small subset of predictions for select dates, locations, and output type IDs, far fewer than an actual modeling hub would typically collect.

The model output data includes quantile, mean and median forecasts of future incident influenza hospitalizations and PMF forecasts of hospitalization intensity. Each forecast is made for five task ID variables, including the location for which the forecast was made (`location`), the date on which the forecast was made (`reference_date`), the number of steps ahead (`horizon`), the date of the forecast prediction (a combination of the date the forecast was made and the forecast horizon, `target_end_date`), and the forecast target (`target`). @tbl-example-model-outputs provides an example set of quantile forecasts included in this example model output. In @tbl-example-model-outputs, we show only the median, the 50%, and 90% prediction intervals, although other intervals and mean forecasts are included in the example model output data.

The [hubExamples]{.pkg} package also provides corresponding target data (@tbl-example-target-data) that contains incident influenza hospitalizations (`observation`) in a given week (`date`) for a given location (`location`). This target data can be used as calibration data for generating forecasts or for evaluating the forecasts post hoc. The forecast-specific task ID variables `reference_date` and `horizon` are not relevant for the target data.

```{r, cols.print=13}
#| label: tbl-example-target-data
#| echo: false
#| tbl-cap: "Example target data for incident influenza hospitalizations. This
#| table includes target data from 2022-11-01 and 2023-02-01. The target data
#| is provided in the [hubExamples]{.pkg} package."
hubExamples::forecast_target_ts |>
  dplyr::filter(
    location == "25",
    date >= "2022-11-01", date <= "2023-02-01"
  ) |>
  knitr::kable(col.names = c("`date`", "`location`", "`observation`"))
```

We can plot these forecasts and the target data using the `plot_step_ahead_model_output()` function from [hubVis]{.pkg}, another package for visualizing model outputs from the hubverse suite (@fig-plot-ex-mods). We subset the model output data and the target data to the location and time horizons we are interested in.

```{r plot-ex-mods}
#| label: fig-plot-ex-mods
#| prompt: true
#| fig-cap: "One example set of quantile forecasts for weekly incident influenza
#| hospitalizations in Massachusetts from each of three models (panels).
#| Forecasts are represented by a median (line), 50% and 90% prediction
#| intervals (ribbons). Gray points represent observed incident hospitalizations."
#| fig-width: 8
#| fig-height: 4
model_outputs_plot <- hubExamples::forecast_outputs |>
    hubUtils::as_model_out_tbl() |>
    dplyr::filter(
      location == "25",
      output_type %in% c("median", "mean", "quantile"),
      reference_date == "2022-12-17"
    )
target_data_plot <- hubExamples::forecast_target_ts |>
    dplyr::filter(
      location == "25",
      date >= "2022-11-01", date <= "2023-02-01"
    )

hubVis::plot_step_ahead_model_output(
    model_out_tbl = model_outputs_plot,
    target_data = target_data_plot,
    facet = "model_id",
    facet_nrow = 1,
    interactive = FALSE,
    intervals = c(0.5, 0.9),
    show_legend = FALSE,
    use_median_as_point = TRUE,
    x_col_name = "target_end_date", 
    x_target_col_name = "date"
  ) +
    theme_bw() +
    labs(y = "incident hospitalizations")
```

Next, we examine the PMF target in the example model output data. For this target, teams forecasted the probability that hospitalization intensity will be "low", "moderate", "high", or "very high". These hospitalization intensity categories are determined by thresholds for weekly hospital admissions per 100,000 population. In other words, "low" hospitalization intensity in a given week means few incident influenza hospitalizations per 100,000 population are predicted, whereas "very high" hospitalization intensity means many hospitalizations per 100,000 population are predicted. These forecasts are made for the same task ID variables as the `quantile` forecasts of incident hospitalizations, other than the target, which is "wk flu hosp rate category" for these categorical predictions. 

```{r, cols.print=9}
#| label: tbl-example-forecasts-pmf
#| echo: false
#| tbl-cap: "Example PMF model output for forecasts of incident influenza
#| hospitalization intensity. A subset of predictions are shown: 1-week
#| ahead PMF forecasts made on 2022-12-17 for Massachusetts from three distinct
#| models. We round the forecasted probability (in the `value` column) to two
#| digits. The `location`, `reference_date` and `target_end_date` columns have
#| been omitted for brevity. This example data is provided in the
#| [hubExamples]{.pkg} package."
hubExamples::forecast_outputs |>
  dplyr::filter(
    output_type %in% c("pmf"),
    reference_date == "2022-12-17",
    location == "25",
    horizon == 1
  ) |>
  dplyr::mutate(value = round(value, 2)) |>
  dplyr::select(all_of(abbr_fc_col)) |>
  knitr::kable(col.names = formatted_fc_col)
```

We show a representative example of the hospitalization intensity category forecasts in @tbl-example-forecasts-pmf. Because these forecasts are PMF output type, the `output_type_id` column specifies the bin of hospitalization intensity and the `value` column provides the forecasted probability of hospitalization incidence being in that category. Values sum to 1 across bins. For the MOBS-GLEAM_FLUH and PSI-DICE models, incidence is forecasted to decrease over the horizon (@fig-plot-ex-mods), and correspondingly, there is lower probability of "high" and "very high" hospitalization intensity for later horizons (@fig-plot-ex-mods-pmf).

```{r}
#| echo: FALSE
#| label: fig-plot-ex-mods-pmf
#| fig-cap: "One example PMF forecast of incident influenza hospitalization
#| intensity is shown for each of three models (panels). Each cell shows the
#| forecasted probability of a given hospitalization intensity bin (low,
#| moderate, high, and very high) for each forecast horizon (0-3 weeks ahead).
#| Darker colors indicate higher forecasted probability."
#| fig-width: 8
#| fig-height: 4
model_outputs_plot_pmf <- hubExamples::forecast_outputs |>
  dplyr::filter(
    location == "25",
    output_type %in% c("pmf"),
    reference_date == "2022-12-17"
  ) |>
  # Reorder bin names to be in correct order
  dplyr::mutate(
    output_type_id = factor(output_type_id,
      levels = c("low", "moderate", "high", "very high")
    )
  )

ggplot(
  data = model_outputs_plot_pmf,
  aes(x = horizon, y = output_type_id, fill = value)
) +
  geom_tile() + 
  facet_grid(cols = vars(model_id)) +
  scale_alpha_discrete(name = "probability") +
  scale_fill_distiller(palette = "YlOrBr", direction = 0, name = "bin probability") +
  scale_x_continuous(expand = c(0, 0), name = "horizon (weeks)") +
  scale_y_discrete(expand = c(0, 0), name = "hospitalization intensity") +
  theme_bw() +
  theme(
    legend.key.width = unit(1,"cm"),
    legend.position = "bottom",
    panel.grid = element_blank()
  )
```

\newpage

### Creating ensembles with simple_ensemble

Using the default options for `simple_ensemble()`, we can generate an equally weighted mean ensemble for each unique combination of values for the task ID variables, the `output_type` and the `output_type_id`. Recall that this function uses different ensemble methods for different output types: for the quantile output type in our example data, the resulting ensemble is a quantile average, while for the PMF output type, the ensemble is a linear pool.

```{r}
#| prompt: true
mean_ens <- hubExamples::forecast_outputs |>
    dplyr::filter(output_type != "sample") |>
    hubEnsembles::simple_ensemble(
      model_id = "simple-ensemble-mean"
    )
```

The resulting model output has the same structure as the original model output data (@tbl-mean-ensemble), with columns for model ID, task ID variables, output type, output type ID, and value. We also use `model_id = "simple-ensemble-mean"` to change the name of this ensemble in the resulting model output; if not specified, the default is "hub-ensemble".

\newpage

```{r, rows.print = 9}
#| label: tbl-mean-ensemble
#| echo: false
#| tbl-cap: "Mean ensemble model output. The values in the `model_id` column are
#| set by the argument `simple_ensemble(..., model_id)`. Results are generated 
#| for all output types, but only a subset are shown: 1-week ahead forecasts made 
#| on 2022-12-17 for Massachusetts, with only the median, 25th and 75th quantiles
#| for the quantile output type and all bins for the PMF output type. The 
#| `location`, `reference_date` and `target_end_date` columns have been omitted 
#| for brevity, and the `value` column is rounded to two digits."
mean_ens |>
  dplyr::filter(
    output_type %in% c("quantile", "median", "pmf"),
    output_type_id %in% c(
      0.025, 0.25, 0.75, 0.975, NA,
      "low", "moderate", "high", "very high"
    ),
    reference_date == "2022-12-17",
    location == "25",
    horizon == 1
  ) |>
  dplyr::mutate(value = round(value, 2)) |>
  dplyr::select(all_of(abbr_fc_col)) |>
  knitr::kable(col.names = formatted_fc_col)
```

#### Changing the aggregation function

We can change the function that is used to aggregate model outputs. For example, we may want to calculate a median of the component models' submitted values for each quantile. We do so by specifying `agg_fun = median`.

```{r}
#| prompt: true
median_ens <- hubExamples::forecast_outputs |>
    dplyr::filter(output_type != "sample") |>
    hubEnsembles::simple_ensemble(
      agg_fun = median,
      model_id = "simple-ensemble-median"
    )
```

Custom functions can also be passed into the `agg_fun` argument. We illustrate this by defining a custom function to compute the ensemble prediction as a geometric mean of the component model predictions. Any custom function to be used must have an argument `x` for the vector of numeric values to summarize, and if relevant, an argument `w` of numeric weights.

```{r}
#| prompt: true
geometric_mean <- function(x) {
    n <- length(x)
    return(prod(x)^(1 / n))
  }
geometric_mean_ens <- hubExamples::forecast_outputs |>
    dplyr::filter(output_type != "sample") |>
    hubEnsembles::simple_ensemble(
      agg_fun = geometric_mean,
      model_id = "simple-ensemble-geometric"
    )
```

As expected, the mean, median, and geometric mean each give us slightly different resulting ensembles. The median point estimates, 50% prediction intervals, and 90% prediction intervals in @fig-plot-ensembles demonstrate this.

```{r plot-ensembles}
#| label: fig-plot-ensembles
#| echo: false
#| fig-cap: "Three different ensembles for weekly incident influenza
#| hospitalizations in Massachusetts. Each ensemble combines individual
#| predictions from the example hub (@fig-plot-ex-mods) using a different
#| method: arithmetic mean, geometric mean, or median. All methods correspond to
#| variations of the quantile average approach. Ensembles are represented by a 
#| median (line), 50% and 90% prediction intervals (ribbons). Geometric mean 
#| ensemble and simple mean ensemble generate similar estimates in this case."
#| fig-height: 4
#| fig-width: 8

model_output_plot <- dplyr::bind_rows(
  mean_ens, median_ens,
  geometric_mean_ens
) |>
  dplyr::filter(
    location == "25",
    output_type %in% c("median", "mean", "quantile"),
    reference_date == "2022-12-17"
  ) |>
  dplyr::mutate(target_date = reference_date + horizon)

target_data_plot <- hubExamples::forecast_target_ts |>
  dplyr::filter(
    location == "25", 
    date >= "2022-11-01", date <= "2023-03-01"
  )

hubVis::plot_step_ahead_model_output(
  model_out_tbl = model_output_plot,
  target_data = target_data_plot,
  use_median_as_point = TRUE,
  interactive = FALSE,
  intervals = c(0.5, 0.90),
  show_legend = TRUE,
  x_col_name = "target_end_date", 
  x_target_col_name = "date"
) +
  theme_bw() +
  labs(y = "incident hospitalizations")
```

#### Weighting model contributions

We can weight the contributions of each model in the ensemble using the `weights` argument of `simple_ensemble()`. This argument takes a `data.frame` that should include a `model_id` column containing each unique model ID and a `weight` column. In the following example, we include the baseline model in the ensemble, but give it less weight than the other forecasts.

```{r}
#| prompt: true
model_weights <- data.frame(
    model_id = c("MOBS-GLEAM_FLUH", "PSI-DICE", "simple_hub-baseline"),
    weight = c(0.4, 0.4, 0.2)
  )
weighted_mean_ens <- hubExamples::forecast_outputs |>
    dplyr::filter(output_type != "sample") |>
    hubEnsembles::simple_ensemble(
      weights = model_weights,
      model_id = "simple-ensemble-weighted-mean"
    )
```

### Creating ensembles with linear_pool

We can also generate a linear pool ensemble, or distributional mixture, using the `linear_pool()` function; this function can be applied to predictions with an `output_type` of mean, quantile, CDF, or PMF. Our example hub includes median output type, so we exclude it from the calculation.

```{r}
#| prompt: true
linear_pool_ens <- hubExamples::forecast_outputs |>
    dplyr::filter(!output_type %in% c("median", "sample")) |>
    hubEnsembles::linear_pool(model_id = "linear-pool")
```

As described above, for `quantile` model outputs, the `linear_pool` function approximates the full probability distribution for each component prediction using the value-quantile pairs provided by that model, and then obtains quasi-random samples from that distributional estimate. The number of samples drawn from the distribution of each component model defaults to `1e4`, but this can be changed using the `n_samples` argument.

In @fig-plot-ex-quantile-and-linear-pool, we compare ensemble results generated by `simple_ensemble()` and `linear_pool()` for model outputs of output types PMF and quantile. As expected, the results from the two functions are equivalent for the PMF output type: for this output type, the `simple_ensemble()` method averages the predicted probability of each category across the component models, which is the definition of the linear pool ensemble method. This is not the case for the quantile output type, because the `simple_ensemble()` is computing a quantile average.

```{r plot-ex-quantile-and-linear-pool}
#| label: fig-plot-ex-quantile-and-linear-pool
#| echo: false
#| fig-cap: "Comparison of results from `linear_pool()` (blue) and
#| `simple_ensemble()` (red). (Panel A) Ensemble predictions of Massachusetts
#| incident influenza hospitalization intensity (classified as low, moderate,
#| high, or very high), which provide an example of PMF output type. (Panel B)
#| Ensemble predictions of weekly incident influenza hospitalizations in
#| Massachusetts, which provide an example of quantile output type. Note, for
#| quantile output type, `simple_ensemble()` corresponds to a quantile average.
#| Ensembles combine individual models from the example hub, and are represented
#| by a median (line), 50% and 90% prediction intervals (ribbons)
#| (@fig-plot-ex-mods)."
#| fig-width: 10
#| fig-height: 4

# Generate plot of ensemble probability in each bin
p1 <- dplyr::bind_rows(mean_ens, linear_pool_ens) |>
  dplyr::filter(
    output_type == "pmf", reference_date == "2022-12-17",
    location == "25"
  ) |>
  # Reorder bin names to be in correct order
  dplyr::mutate(
    output_type_id = 
      factor(
        output_type_id,
        levels = c("low", "moderate", "high", "very high")
      )
  ) |>
  ggplot(aes(x = output_type_id, y = value, fill = model_id)) +
  geom_bar(stat = "identity", position = "dodge") +
  facet_wrap(vars(horizon), labeller = label_both) +
  labs(x = "incident hospitalization intensity", y = "probability") +
  scale_fill_brewer(palette = "Set1", direction = -1) +
  theme_bw() +
  theme(
    legend.position = "bottom", legend.title = element_blank(),
    strip.background = element_blank(), strip.placement = "outside",
    panel.grid.major.x = element_blank()
  )

## Create new data frame of ensemble output to be plotted
# Use median results as point estimate in plot
# Pull quantile == 0.5 results for linear pool and add them to the end of 
# the data.frame as "median" output type (because linear_pool() does not
# automatically provide median estimates)
model_output_plot <- linear_pool_ens |>
  dplyr::filter(output_type_id == 0.5, target == "wk inc flu hosp") |>
  dplyr::mutate(output_type = "median", output_type_id = NA)
model_output_plot <- dplyr::bind_rows(linear_pool_ens, model_output_plot)
# Filter to desired forecasts
model_output_plot <- dplyr::bind_rows(mean_ens, model_output_plot) |>
  dplyr::filter(
    location == "25",
    output_type %in% c("median", "mean", "quantile"),
    reference_date == "2022-12-17"
  ) |>
  dplyr::mutate(
    target_date = reference_date + horizon, 
    output_type_id = as.numeric(output_type_id)
  )
target_data_plot <- hubExamples::forecast_target_ts |>
  dplyr::filter(
    location == "25", 
    date >= "2022-11-01", date <= "2023-03-01"
  )

# Generate plot of incidence forecasts 0-3 week horizons
p2 <-
  hubVis::plot_step_ahead_model_output(
    model_out_tbl = model_output_plot,
    target_data = target_data_plot,
    use_median_as_point = TRUE,
    interactive = FALSE,
    intervals = c(0.5, 0.9),
    pal_color = "Set1",
    show_legend = TRUE,
    x_col_name = "target_end_date", 
    x_target_col_name = "date"
  ) +
  theme_bw() +
  labs(y = "incident hospitalizations")

# Pull legend
l <- cowplot::get_plot_component(p1, "guide-box", return_all = TRUE)[[3]]

# Assemble plots
cowplot::plot_grid(
  cowplot::plot_grid(
    p1 + labs(subtitle = "example PMF output type") +
      theme(legend.position = "none"),
    p2 + labs(subtitle = "example quantile output type") +
      theme(legend.position = "none"),
    labels = LETTERS[1:2]
  ), l,
  ncol = 1,
  rel_heights = c(0.95, 0.05)
)
```


## Example: in-depth analysis of forecast data {#sec-case-study}

To further demonstrate the utility of the [hubEnsembles]{.pkg} package and the differences between the two ensembling functions, we examine a more complex example. Unlike the previous section's basic showcase of functionality, we use this case study to provide a more complete analysis that compares and evaluates ensemble model performance using real forecasts collected by a modeling hub, with an overarching goal of choosing a single best ensembling approach for the application.

Since 2013, the US Centers for Disease Control and Prevention (CDC) has been soliciting forecasts of seasonal influenza from modeling teams through a collaborative challenge called FluSight [@cdc_flusight]. We use a subset of these predictions to create four equally-weighted ensembles with `simple_ensemble()` and `linear_pool()` and compare the resulting ensembles' performance. The ensembling methods chosen for this case study consist of a quantile (arithmetic) mean, a quantile median, a linear pool with normal tails, and a linear pool with lognormal tails. Note that only a select portion of the code is shown in this manuscript for brevity, although all the functions and scripts used to generate the case study results can be found in the associated GitHub repository (<https://github.com/hubverse-org/hubEnsemblesManuscript>). More specifically, the figures and tables supporting this analysis are generated reproducibly using data from rds files stored in the `analysis/data/raw-data` directory and scripts in the `inst` directory of the repository.

### Data and Methods

We begin by querying the component forecasts used to generate the four ensembles from Zoltar [@reich_zoltar_2021], a repository designed to archive forecasts created by the Reich Lab at UMass Amherst. For this analysis we only consider FluSight predictions in a quantile format from the 2021-2022 and 2022-2023 seasons. These forecasts were stored in two data objects, split by season, called `flu_forecasts-zoltar_21-22.rds` and `flu_forecasts-zoltar_22-23.rds`, and a subset is shown below in @tbl-raw-flu-forecasts.

```{r read in raw data, eval=TRUE}
#| prompt: true
flu_forecasts_raw_21_22 <- readr::read_rds(
    here::here("analysis/data/raw_data/flu_forecasts-zoltar_21-22.rds")
  )
flu_forecasts_raw_22_23 <- readr::read_rds(
    here::here("analysis/data/raw_data/flu_forecasts-zoltar_22-23.rds")
  )
flu_forecasts_raw <- rbind(flu_forecasts_raw_21_22, flu_forecasts_raw_22_23)
```

```{r raw flu forecasts, message=FALSE, warning=FALSE, echo=FALSE}
#| label: tbl-raw-flu-forecasts
#| tbl-cap: "An example prediction of weekly incident influenza
#| hospitalizations pulled directly from Zoltar. The example forecasts were
#| made on May 15, 2023 for California at the 1 week ahead horizon. The
#| forecasts were generated during the FluSight forecasting challenge, then
#| formatted according to Zoltar standards for storage. The `timezero`, 
#| `season`, `unit`, `param1`, `param2`, and `param3` columns have been
#| omitted for brevity. (The `season` column has a value of '2021-2022' or 
#| '2022-2023' while the last three 'param' columns always have a value
#| of NA.)"
zoltar_col <- formatted_zoltar_col <- c("model", "target", "class", "value", "cat", "prob", "sample", "quantile", "family")
for (i in 1:length(zoltar_col)) formatted_zoltar_col[i] <- paste0("`", zoltar_col[i], "`")

flu_forecasts_raw |>
  dplyr::filter(
    model == "UMass-trends_ensemble",
    timezero == "2023-05-15",
    unit == "06",
    target == "1 wk ahead inc flu hosp",
    quantile %in% c(0.025, 0.1, 0.25, 0.75, 0.9, 0.975)
  ) |>
  dplyr::select(all_of(zoltar_col)) |>
  knitr::kable(col.names = formatted_zoltar_col)
```

Forecasts must conform to hubverse standards to be fed into either of the ensembling functions, so we first transform the raw forecasts using the  `as_model_out_tbl()`[^1] function from the [hubUtils]{.pkg} package. Here, we specify the task ID variables `forecast_date` (when the forecast was made), `location`, `horizon`, and `target`.

[^1]: https://hubverse-org.github.io/hubUtils/reference/as_model_out_tbl.html

```{r transform data, eval=TRUE}
#| prompt: true
flu_forecasts_hubverse <- flu_forecasts_raw |>
    dplyr::rename(forecast_date = timezero, location = unit) |>
    tidyr::separate(target, sep = " ", convert = TRUE,
                    into = c("horizon", "target"), extra = "merge") |>
    dplyr::mutate(target_end_date = 
                    ceiling_date(forecast_date + weeks(horizon), "weeks") -
                      days(1)) |>
    as_model_out_tbl(
      model_id_col = "model",
      output_type_col = "class",
      output_type_id_col = "quantile",
      value_col = "value",
      sep = "-",
      trim_to_task_ids = FALSE,
      hub_con = NULL,
      task_id_cols = 
        c("forecast_date", "location", "horizon", "target", target_end_date),
      remove_empty = TRUE
    )
```

Prior to ensemble calculation (shown later in this section), we filter out any predictions (defined by a unique combination of task ID variables) that did not include all 23 quantiles specified by FluSight ($\theta \in \{.010, 0.025, .050, .100, ..., .900, .950, .990\}$). The FluSight baseline and median ensemble models generated by the FluSight hub are also excluded from the component forecasts. We chose to remove the baseline to match the composition of models used to create the official FluSight ensemble.

With these inclusion criteria, the final data set of component forecasts consists of predictions from 25 modeling teams and 42 distinct models, 53 forecast dates (one per week), 54 US locations, 4 horizons, 1 target, and 23 quantiles. In the 2021-2022 season, 25 models made predictions for 22 weeks spanning from late January 2022 to late June 2022, and in the 2022-2023 season, there were 31 models making predictions for 31 weeks spanning mid-October 2022 to mid-May 2023. Fourteen of the 42 total models made forecasts for both seasons.

In both seasons, forecasts were made for the same locations (the 50 US states, Washington DC, Puerto Rico, the Virgin Islands, and the US as a whole), horizons (1 to 4 weeks ahead), quantiles (the 23 described above), and target (week ahead incident flu hospitalization). The values for the forecasts are always non-negative. In @tbl-case-study-flu-forecasts, we provide an example of these predictions, showing select quantiles from a single model, forecast date, horizon, and location.

```{r formatted flu forecasts, eval=TRUE}
#| label: tbl-case-study-flu-forecasts
#| echo: false
#| tbl-cap: "An example prediction of weekly incident influenza
#| hospitalizations. The example model output was made on May 15, 2023 for
#| California at the 1 week ahead horizon. The forecast was generated during
#| the FluSight forecasting challenge, then formatted according to hubverse
#| standards post hoc. The `location`, `forecast_date`, and `season` columns
#| have been omitted for brevity; quantiles representing the endpoints of the 
#| central 50%, 80% and 95% prediction intervals are shown."
flu_forecasts_hubverse |>
  dplyr::filter(
    model_id == "UMass-trends_ensemble",
    forecast_date == "2023-05-15",
    location == "06",
    horizon == 1,
    output_type_id %in% c(0.025, 0.1, 0.25, 0.75, 0.9, 0.975)
  ) |>
  dplyr::select(all_of(abbr_fc_col)) |>
  knitr::kable(col.names = formatted_fc_col)
```

Next, we combine the component model outputs to generate predictions from each ensemble model. We begin by excluding the baseline model from the set of predictions that will be combined. Then, we create one object to store the ensemble results generated from each method we are interested in comparing.

```{r construct ensembles, eval=FALSE}
#| prompt: true
flu_forecasts_component <- dplyr::filter(
    flu_forecasts_hubverse,
    !model_id %in% c("Flusight-baseline", "Flusight-ensemble")
  )

mean_ensemble <- flu_forecasts_component |>
    hubEnsembles::simple_ensemble(
      weights = NULL,
      agg_fun = mean,
      model_id = "mean-ensemble"
    )
median_ensemble <- flu_forecasts_component |>
    hubEnsembles::simple_ensemble(
      weights = NULL,
      agg_fun = median,
      model_id = "median-ensemble"
    )
lp_normal <- flu_forecasts_component |>
    hubEnsembles::linear_pool(
      weights = NULL,
      n_samples = 1e5,
      model_id = "lp-normal",
      tail_dist = "norm"
    )
lp_lognormal <- flu_forecasts_component |>
    hubEnsembles::linear_pool(
      weights = NULL,
      n_samples = 1e5,
      model_id = "lp-lognormal",
      tail_dist = "lnorm"
    ) 
```

```{r write ensembles, eval=FALSE, echo=FALSE}
# save forecasts
readr::write_rds(
  mean_ensemble,
  here::here("analysis/data/derived_data/flu_mean-ensemble_forecasts.rds"), 
  "xz", compression = 9L
)
readr::write_rds(
  median_ensemble,
  here::here("analysis/data/derived_data/flu_median-ensemble_forecasts.rds"), 
  "xz", compression = 9L
)
readr::write_rds(
  lp_normal,
  here::here("analysis/data/derived_data/flu_lp-normal_forecasts.rds"), 
  "xz", compression = 9L
)
readr::write_rds(
  lp_lognormal,
  here::here("analysis/data/derived_data/flu_lp-lognormal_forecasts.rds"), 
  "xz", compression = 9L
)
```

We evaluate the performance of these ensembles using scoring metrics that measure the accuracy and calibration of their forecasts. Here, we choose several common metrics in forecast evaluation, including mean absolute error (MAE), weighted interval score (WIS) [@bracher_evaluating_2021], 50% prediction interval (PI) coverage, and 95% PI coverage. MAE measures the average absolute error of a set of point forecasts; smaller values of MAE indicate better forecast accuracy. WIS is a generalization of MAE for probabilistic forecasts and is an alternative to other common proper scoring rules which cannot be evaluated directly for quantile forecasts [@bracher_evaluating_2021]. WIS is made up of three component penalties: (1) for over-prediction, (2) for under-prediction, and (3) for the spread of each interval (where an interval is defined by a symmetric set of two quantiles). This metric weights these penalties across all prediction intervals provided. A lower WIS value indicates a more accurate forecast [@bracher_evaluating_2021]. PI coverage provides information about whether a forecast has accurately characterized its uncertainty about future observations. The $50$% PI coverage rate measures the proportion of the time that 50% prediction intervals at that nominal level included the observed value; the 95% PI coverage rate is defined similarly. Achieving approximately nominal (50% or 95%) coverage indicates a well-calibrated forecast.

```{r read in forecasts and scores, echo=FALSE}
flu_truth_all <- readr::read_rds(
  here::here("analysis/data/raw_data/flu_truth-zoltar.rds")
) |>
  dplyr::rename(location = unit, forecast_date = timezero) |>
  dplyr::mutate(model = "flu-truth", target_variable = "inc flu hosp",
                target_end_date = ceiling_date(forecast_date, "weeks") - days(1)) |>
  dplyr::select(model, target_variable, target_end_date, location, value)
  
flu_files <- list.files(
  path = here::here("analysis/data/derived_data"),
  pattern = "forecasts",
  full.names = TRUE
)
flu_ensembles_forecasts <- purrr::map_dfr(flu_files, .f = readr::read_rds)

flu_baseline_forecasts <- flu_forecasts_hubverse |>
  dplyr::filter(model_id == "Flusight-baseline")
flu_baseline_scores <- readr::read_rds(
  here::here("analysis/data/derived_data/flu_baseline_scores.rds")
)
flu_ensembles_scores <- readr::read_rds(
  here::here("analysis/data/derived_data/flu_ensembles_scores.rds")
)
flu_scores_all <- rbind(flu_ensembles_scores, flu_baseline_scores)
```

We also use relative versions of WIS and MAE (rWIS and rMAE, respectively) to understand how the ensemble performance compares to that of the FluSight baseline model. These metrics are calculated as $$\textrm{rWIS} = \frac{\textrm{WIS}_{\textrm{model }m}}{\textrm{WIS}_{\textrm{baseline}}} \hspace{3cm} \textrm{rMAE} = \frac{\textrm{MAE}_{\textrm{model }m}}{\textrm{MAE}_{\textrm{baseline}}},$$ where model $m$ is any given model being compared against the baseline. For both of these metrics, a value less than one indicates better performance compared to the baseline while a value greater than one indicates worse performance. By definition, the FluSight baseline itself will always have a value of one for both of these metrics.

Each unique prediction from an ensemble model is scored against target data using the `score_forecasts()`[^2] function from the [covidHubUtils]{.pkg} package, as a hubverse package for scoring and evaluation has not yet been fully implemented. This function outputs each of the metrics described above. We use median forecasts taken from the 0.5 quantile for the MAE evaluation.

[^2]: https://reichlab.io/covidHubUtils/reference/score_forecasts.html

### Performance results across ensembles

The quantile median ensemble has the best overall performance in terms of WIS and MAE (and the relative versions of these metrics), and has coverage rates that were close to the nominal levels (@tbl-overall-evaluation). The two linear opinion pools have very similar performance to each other. These methods have the second-best performance as measured by WIS and MAE, but they have the highest 50% and 95% coverage rates, with empirical coverage that was well above the nominal coverage rate. The quantile mean performs the worst of the ensembles with the highest MAE, which is substantially different from that of the other ensembles.

```{r overall-evaluation, message=FALSE, warning=FALSE, echo=FALSE}
#| label: tbl-overall-evaluation
#| tbl-cap: "Summary of overall model performance across both seasons, averaged
#| over all locations except the US national location and sorted by ascending
#| WIS. The quantile median ensemble has the best value for every metric except
#| 50% coverage rate, though metric values are often quite similar among
#| the models."

metrics_col <- formatted_metrics_col <- c("model", "wis", "rwis", "mae", "rmae", "cov50", "cov95")
for (i in 1:length(metrics_col)) formatted_metrics_col[i] <- paste0("`", metrics_col[i], "`")

flu_overall_states <- flu_scores_all |>
  evaluate_flu_scores(
    grouping_variables = NULL,
    baseline_name = "Flusight-baseline", us_only = FALSE
  )

flu_overall_states |>
  dplyr::select(all_of(metrics_col)) |>
  knitr::kable(col.names = formatted_metrics_col)
```

Plots of the models' forecasts can aid our understanding about the origin of these accuracy differences. For example, the linear opinion pools consistently have some of the widest prediction intervals, and consequently the highest coverage rates. The median ensemble, which has the best WIS, balanced interval width with calibration best overall, with narrower intervals than the linear pools that still achieved near-nominal coverage on average across all time points. The quantile mean's interval widths vary, though it usually has narrower intervals than the linear pools. However, this model's point forecasts have a larger error margin compared to the other ensembles, especially at longer horizons. This pattern is demonstrated in @fig-plot-forecasts-hubVis for the 4-week ahead forecast in California following the 2022-23 season peak on December 5, 2022. Here, the quantile mean predicted a continued increase in hospitalizations, at a steeper slope than the other ensemble methods.

```{r plot-forecasts-hubVis, warning=FALSE, message=FALSE}
#| label: fig-plot-forecasts-hubVis
#| echo: false
#| fig-cap: "One to four week ahead forecasts for select dates plotted against
#| target data for California. The first panel shows all models on the same 
#| scale. All other panels show forecasts for each individual model, with
#| varying y-axis scales, and their prediction accuracy as compared to
#| observed influenza hospitalizations."
#| fig-width: 9
#| fig-height: 12

model_names <- c(
  "Flusight-baseline", "lp-lognormal", "lp-normal",
  "mean-ensemble", "median-ensemble"
)

flu_dates_21_22 <- as.Date("2022-01-24") + weeks(0:21)
flu_dates_22_23 <- as.Date("2022-10-17") + weeks(0:30)
all_flu_dates <- c(flu_dates_21_22, flu_dates_22_23)

forecasts_ca <- flu_ensembles_forecasts |>
  rbind(flu_baseline_forecasts) |>
  dplyr::filter(
    location == "06",
    forecast_date %in% all_flu_dates[seq(1, 69, 4)] # dates once every 4 weeks
  ) |>
  dplyr::group_by(forecast_date)

# Add non-forecasted weeks with NA value 
# prevents erroneous line connecting seasons
truth_ca <- flu_truth_all |>
  dplyr::filter(location == "06") |>
  dplyr::rename(observation = value) |>
  rbind(expand.grid(
    model = "flu-truth",
    target_variable = "inc flu hosp",
    target_end_date = flu_dates_21_22[22] + lubridate::weeks(1:16),
    location = unique(flu_truth_all$location),
    observation = NA
  ))


# Plot ensembles' forecasts only
ca_plot_ensembles <-
  plot_step_ahead_model_output(
    forecasts_ca |> filter(model_id != "Flusight-baseline"),
    truth_ca,
    use_median_as_point = TRUE,
    show_plot = FALSE,
    x_col_name = "target_end_date",
    x_target_col_name = "target_end_date",
    show_legend = FALSE,
    facet = "model_id",
    facet_scales = "free_y",
    facet_nrow = 3,
    interactive = FALSE,
    pal_color = "Set2",
    fill_transparency = 0.45,
    intervals = c(0.5, 0.95),
    title = NULL,
    group = "forecast_date"
  )

ca_plot_ensembles <- ca_plot_ensembles +
  scale_x_date(
    name = NULL, limits = c(
      as.Date("2022-01-01"),
      as.Date("2023-06-08")
    ),
    date_breaks = "4 months", date_labels = "%b '%y"
  ) +
  scale_color_manual(
    breaks = model_names[2:5],
    values = RColorBrewer::brewer.pal(5, "Set2")[2:5]
  ) +
  scale_fill_manual(
    breaks = model_names[2:5],
    values = RColorBrewer::brewer.pal(5, "Set2")[2:5]
  ) +
  theme(
    axis.ticks.length.x = unit(0.5, "cm"),
    axis.text.x = element_text(vjust = 7, hjust = -0.2),
    axis.title.y = element_blank(),
    legend.position = "none"
  )

# Plot separate baseline "facet"
ca_plot_baseline <-
  plot_step_ahead_model_output(
    forecasts_ca |> filter(model_id == "Flusight-baseline"),
    truth_ca,
    use_median_as_point = TRUE,
    show_plot = TRUE,
    x_col_name = "target_end_date",
    x_target_col_name = "target_end_date",
    show_legend = FALSE,
    facet = "model_id",
    facet_scales = "free_y",
    facet_nrow = 1,
    interactive = FALSE,
    fill_transparency = 0.45,
    intervals = c(0.5, 0.95),
    title = NULL,
    group = "forecast_date"
  ) +
  scale_x_date(
    name = NULL, limits = c(
      as.Date("2022-01-01"),
      as.Date("2023-06-08")
    ),
    date_breaks = "4 months", date_labels = "%b '%y"
  ) +
  theme_bw() +
  theme(
    axis.title.x = element_blank(),
    axis.ticks.length.x = unit(0, "cm"),
    axis.text.x = element_blank(),
    axis.title.y = element_blank(),
    legend.position = "none"
  )

# Plot all models on same graph (for scale comparison)
ca_plot_all <-
  plot_step_ahead_model_output(
    forecasts_ca |> mutate(facet_name = "All models, same scale"),
    truth_ca,
    use_median_as_point = TRUE,
    show_plot = TRUE,
    x_col_name = "target_end_date",
    x_target_col_name = "target_end_date",
    show_legend = FALSE,
    facet = "facet_name",
    interactive = FALSE,
    fill_transparency = 0.45,
    intervals = c(0.5, 0.95),
    title = NULL,
    group = "forecast_date"
  ) +
  scale_x_date(
    name = NULL, limits = c(
      as.Date("2022-01-01"),
      as.Date("2023-06-08")
    ),
    date_breaks = "4 months", date_labels = "%b '%y"
  ) +
  theme_bw() +
  theme(
    axis.title.x = element_blank(),
    axis.text.x = element_blank(),
    axis.ticks.length.x = unit(0, "cm"),
    axis.title.y = element_blank(),
    legend.position = "none"
  )

# Assemble plots
((ca_plot_all | ca_plot_baseline) / ca_plot_ensembles) +
  theme_bw() +
  plot_layout(guides = "collect", heights = c(1, 2)) +
  plot_annotation(title = paste0(
    "Weekly Incident ",
    "Hospitalizations for Influenza ",
    "in California"
  ))
```

Averaging across all time points, the median model can be seen to have the best scores for every metric. It outperforms the mean ensemble by a similar amount for both MAE and WIS, particularly around local times of change (see @fig-mae-vs-forecast-date and @fig-wis-vs-forecast-date). The median ensemble also has better coverage rates than the mean ensemble in the tails of the distribution (95% intervals, see @fig-cov95-vs-forecast-date) and similar coverage in the center (50% intervals). The median model also outperforms the linear pools for most weeks, with the greatest differences in scores being for WIS and coverage rates (@fig-wis-vs-forecast-date and @fig-cov95-vs-forecast-date). This seems to indicate that the linear pools' estimates are usually too conservative, with their wide intervals and higher-than-nominal coverage rates being penalized by WIS. However, during the 2022-2023 season there are several localized times when the linear pools showcased better one-week-ahead forecasts than the median ensemble (@fig-wis-vs-forecast-date). These localized instances are characterized by similar MAE values (@fig-wis-vs-forecast-date) for the two methods and poor median ensemble coverage rates (@fig-cov95-vs-forecast-date). In these instances, the wide intervals from the linear pools were useful in capturing the eventually-observed hospitalizations, usually during times of rapid change.

```{r mae-vs-forecast-date, message=FALSE, warning=FALSE, echo=FALSE}
#| label: fig-mae-vs-forecast-date
#| prompt: true
#| fig-cap: "Mean absolute error (MAE) averaged across all locations. Average
#| target data across all locations for 2021-2022 (A) and 2022-2023 (B)
#| seasons for reference. For each season, average MAE is shown for 1-week (C-D)
#| and 4-week ahead (E-F) forecasts. Results are plotted for each ensemble model 
#| (colors) across the entire season. Lower values indicate better performance."
#| fig-width: 10
#| fig-height: 12
model_names <- c(model_names, "average target data")
model_colors <- c(RColorBrewer::brewer.pal(5, "Set2"), "black")

# Summarize flu scores by horizon, forecast_date, and season
flu_date_horizon_season_states <- flu_scores_all |>
  evaluate_flu_scores(
    grouping_variables = c("horizon", "forecast_date", "season"),
    baseline_name = "Flusight-baseline", us_only = FALSE
  )

# Plot truth data, faceted by season
truth_states_2122 <- flu_truth_all |>
  plot_flu_truth(
    date_range = flu_dates_21_22[c(1, 22)],
    main = "Target Data 2021-22 (wk inc flu hosp)"
  )
truth_states_2223 <- flu_truth_all |>
  plot_flu_truth(
    date_range = flu_dates_22_23[c(1, 31)],
    main = "Target Data 2022-23 (wk inc flu hosp)"
  )

mae_date_plot_states1_2122 <- flu_date_horizon_season_states |>
  dplyr::filter(season == "2021-2022") |>
  plot_evaluated_scores_forecast_date(
    model_names, model_colors,
    h = 1, y_var = "mae",
    main = "MAE 2021-2022, 1 week ahead"
  )
mae_date_plot_states4_2122 <- flu_date_horizon_season_states |>
  dplyr::filter(season == "2021-2022") |>
  plot_evaluated_scores_forecast_date(
    model_names, model_colors,
    h = 4, y_var = "mae",
    main = "MAE 2021-2022, 4 week ahead"
  )

# Plot MAE by forecast_date, faceted by horizon and season
mae_date_plot_states1_2223 <- flu_date_horizon_season_states |>
  dplyr::filter(season == "2022-2023") |>
  plot_evaluated_scores_forecast_date(
    model_names, model_colors,
    h = 1, y_var = "mae",
    main = "MAE 2022-2023, 1 week ahead"
  )
mae_date_plot_states4_2223 <- flu_date_horizon_season_states |>
  dplyr::filter(season == "2022-2023") |>
  plot_evaluated_scores_forecast_date(
    model_names, model_colors,
    h = 4, y_var = "mae",
    main = "MAE 2022-2023, 4 week ahead"
  )

# Assemble plots into larger MAE figure
truth_states_2122 + truth_states_2223 +
  mae_date_plot_states1_2122 + mae_date_plot_states1_2223 +
  mae_date_plot_states4_2122 + mae_date_plot_states4_2223 +
  theme_bw() +
  plot_annotation(tag_levels = "A") + 
  plot_layout(ncol = 2, guides = "collect", heights = c(1, 2, 2)) &
  theme(legend.position = "bottom", 
        plot.tag = element_text(face = 'bold'))
```

```{r wis-vs-forecast-date, message=FALSE, warning=FALSE, echo=FALSE}
#| label: fig-wis-vs-forecast-date
#| prompt: true
#| fig-cap: "Weighted interval score (WIS) averaged across all locations.
#| Average target data across all locations for 2021-2022 (A) and 2022-2023 (B)
#| seasons for reference. For each season, average WIS is shown for 1-week (C-D)
#| and 4-week ahead (E-F) forecasts. Results are plotted for each ensemble model 
#| (colors) across the entire season. Lower values indicate better performance."
#| 
#| fig-width: 10
#| fig-height: 12

# Plot WIS by forecast_date, faceted by horizon and season
wis_date_plot_states1_2122 <- flu_date_horizon_season_states |>
  dplyr::filter(season == "2021-2022") |>
  plot_evaluated_scores_forecast_date(
    model_names, model_colors,
    h = 1, y_var = "wis",
    main = "WIS 2021-2022, 1 week ahead"
  )
wis_date_plot_states4_2122 <- flu_date_horizon_season_states |>
  dplyr::filter(season == "2021-2022") |>
  plot_evaluated_scores_forecast_date(
    model_names, model_colors,
    h = 4, y_var = "wis",
    main = "WIS 2021-2022, 4 week ahead"
  )

wis_date_plot_states1_2223 <- flu_date_horizon_season_states |>
  dplyr::filter(season == "2022-2023") |>
  plot_evaluated_scores_forecast_date(
    model_names, model_colors,
    h = 1, y_var = "wis",
    main = "WIS 2022-2023, 1 week ahead"
  )
wis_date_plot_states4_2223 <- flu_date_horizon_season_states |>
  dplyr::filter(season == "2022-2023") |>
  plot_evaluated_scores_forecast_date(
    model_names, model_colors,
    h = 4, y_var = "wis",
    main = "WIS 2022-2023, 4 week ahead"
  )

# Assemble plots into larger WIS figure
truth_states_2122 + truth_states_2223 +
  wis_date_plot_states1_2122 + wis_date_plot_states1_2223 +
  wis_date_plot_states4_2122 + wis_date_plot_states4_2223 +
  theme_bw() +
  plot_annotation(tag_levels = "A") + 
  plot_layout(ncol = 2, guides = "collect", heights = c(1, 2, 2)) &
  theme(legend.position = "bottom", 
        plot.tag = element_text(face = 'bold'))
```

```{r cov95-vs-forecast-date, message=FALSE, warning=FALSE, echo=FALSE}
#| label: fig-cov95-vs-forecast-date
#| prompt: true
#| fig-cap: "95% prediction interval (PI) coverage averaged across all
#| locations. Average target data across all locations for 2021-2022 (A) 
#| and 2022-2023 (B) seasons for reference. For each season, average coverage is
#| shown for 1-week (C-D) and 4-week ahead (E-F) forecasts. Results are plotted
#| for each ensemble model (colors) across the entire season. Ideal coverage of
#| 95% is shown (black horizontal line); values closer to 95% indicate better 
#| performance."
#| fig-width: 9
#| fig-height: 12

# Plot 95% coverage by forecast_date, faceted by horizon and season
cov95_date_plot_states1_2122 <- flu_date_horizon_season_states |>
  dplyr::filter(season == "2021-2022") |>
  plot_evaluated_scores_forecast_date(
    model_names, model_colors,
    h = 1, y_var = "cov95",
    main = "95% Coverage 2021-22, 1 week ahead"
  ) +
  coord_cartesian(ylim = c(0, 1.05))
cov95_date_plot_states4_2122 <- flu_date_horizon_season_states |>
  dplyr::filter(season == "2021-2022") |>
  plot_evaluated_scores_forecast_date(
    model_names, model_colors,
    h = 4, y_var = "cov95",
    main = "95% Coverage 2021-22, 4 week ahead"
  ) +
  coord_cartesian(ylim = c(0, 1.05))

cov95_date_plot_states1_2223 <- flu_date_horizon_season_states |>
  dplyr::filter(season == "2022-2023") |>
  plot_evaluated_scores_forecast_date(
    model_names, model_colors,
    h = 1, y_var = "cov95",
    main = "95% Coverage 2022-23, 1 week ahead"
  ) +
  coord_cartesian(ylim = c(0, 1.05))
cov95_date_plot_states4_2223 <- flu_date_horizon_season_states |>
  dplyr::filter(season == "2022-2023") |>
  plot_evaluated_scores_forecast_date(
    model_names, model_colors,
    h = 4, y_var = "cov95",
    main = "95% Coverage 2022-23, 4 week ahead"
  ) +
  coord_cartesian(ylim = c(0, 1.05))

# Assemble plots into larger 95% coverage figure
truth_states_2122 + truth_states_2223 +
  cov95_date_plot_states1_2122 + cov95_date_plot_states1_2223 +
  cov95_date_plot_states4_2122 + cov95_date_plot_states4_2223 +
  theme_bw() +
  plot_annotation(tag_levels = "A") + 
  plot_layout(ncol = 2, guides = "collect", heights = c(1, 2, 2)) &
  theme(legend.position = "bottom", 
        plot.tag = element_text(face = 'bold'))
```

In this analysis, all of the ensemble variations outperform the baseline model; yet, different ensembling methods perform best under different circumstances. While the quantile median has the best overall results for WIS, MAE, 50% PI coverage, and 95% PI coverage, other models may perform better from week-to-week for each metric. Around the 2022-2023 season's peak in early December, the remaining four models (including the baseline) each have instances in which they achieve the lowest WIS, like the linear pool ensembles for the one week ahead horizon over several weeks of this period.

The choice of an appropriate ensemble aggregation method may depend on the forecast target, the goal of forecasting, and the behavior of the individual models contributing to an ensemble. One case may call for prioritizing high coverage rates while another may prioritize accurate point forecasts. The `simple_ensemble()` and `linear_pool()` functions and the ability to specify component model weights and an aggregation function for `simple_ensemble()` allow users to implement a variety of ensemble methods.

## Summary and discussion {#sec-conclusions}

Ensembles of independent models are a powerful tool to generate more accurate and more reliable predictions of future outcomes than a single model alone. Here, we have demonstrated how to utilize [hubEnsembles]{.pkg}, a simple and flexible framework to combine individual model predictions into an ensemble.

The [hubEnsembles]{.pkg} package is situated within the larger hubverse collection of open-source software and data tools to support collaborative modeling exercises [@hubverse_docs]. Collaborative hubs offer many benefits, including serving as a centralized entity to guide and elicit predictions from multiple independent models [@reich2022]. Given the increasing popularity of multi-model ensembles and collaborative hubs, there is a clear need for generalized data standards and software infrastructure to support these hubs. By addressing this need, the hubverse suite of tools can reduce duplicative efforts across existing hubs, support other communities engaged in collaborative efforts, and enable the adoption of multi-model approaches in new domains.

When using [hubEnsembles]{.pkg}, it is important to carefully choose an ensemble method that is well suited for the situation. Although there may not be a universal "best" method, matching the properties of a given ensemble method with the features of the component models will likely yield best results [@howerton2023]. Our case study on seasonal influenza forecasts in the US demonstrates this point. The quantile median ensemble performs best overall for a range of metrics, including weighted interval score, mean absolute error, and prediction interval coverage. Yet, the linear pool method, which generates an ensemble with wider prediction intervals, demonstrates performance advantages during periods of rapid change, when outlying component forecasts are likely more important. Notably, all ensemble methods outperform the baseline model. The performance improvements from ensemble models motivate the use of a "hub-based" approach to prediction for infectious diseases and in other fields.

Ongoing development of the [hubEnsembles]{.pkg} package and the larger suite of hubverse tools will continue to support multi-model predictions in new ways, including for example supporting additional types of predictions, enabling scoring and evaluation of those predictions, and allowing for cloud-based data storage. All such infrastructure will ultimately provide a comprehensive suite of open-source software tools for leveraging the power of collaborative hubs and multi-model ensembles.

## Acknowledgements {.unnumbered}

The authors thank all members of the hubverse community; the broader hubverse software infrastructure made this package possible. L. Shandross, A. Krystalli, N. G. Reich, and E. L. Ray were supported by the National Institutes of General Medical Sciences (R35GM119582) and the US Centers for Disease Control and Prevention (U01IP001122 and NU38FT000008). E. Howerton was supported by NSF RAPID awards DEB-2126278 and DEB-2220903, as well as the Eberly College of Science Barbara McClintock Science Achievement Graduate Scholarship in Biology at the Pennsylvania State University. L. Contamin and H. Hochheiser were supported by NIGMS grant U24GM132013. The content is solely the responsibility of the authors and does not necessarily represent the official views of NIGMS, the National Institutes of Health, or CDC.

## Consortium of Infectious Disease Modeling Hubs {.unnumbered}

Consortium of Infectious Disease Modeling Hubs authors include Alvaro J. Castro Rivadeneira (University of Massachusetts Amherst), Lucie Contamin (University of Pittsburgh), Sebastian Funk (London School of Hygiene & Tropical Medicine), Aaron Gerding (University of Massachusetts Amherst), Hugo Gruson (data.org), Harry Hochheiser (University of Pittsburgh), Emily Howerton (The Pennsylvania State University), Melissa Kerr (University of Massachusetts Amherst), Anna Krystalli (R-RSE SMPC), Sara L. Loo (Johns Hopkins University), Evan L. Ray (University of Massachusetts Amherst), Nicholas G. Reich (University of Massachusetts Amherst), Koji Sato (Johns Hopkins University), Li Shandross (University of Massachusetts Amherst), Katharine Sherratt (London School of Hygene and Tropical Medicine), Shaun Truelove (Johns Hopkins University), Martha Zorn (University of Massachusetts Amherst)

## References {.unnumbered}
